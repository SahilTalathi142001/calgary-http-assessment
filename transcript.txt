00:00 Hello, my name is Sahil Talathi, and in this video, I will be walk through, walk you through, with the project on web server raw analysis using Python.
00:15 I worked with, , Calgary HTTP dataset and performed data parsing, cleaning, and expedited analysis, to extract meaningful insights from the raw web server log data.
00:34 In the raw web server logs, in this data set, data set, I used details like IP address, timestamp, HTTP request, response code, and byte size.
00:46 So, so the goal of this project was to analyze the traffic patterns, errors, caused by the file name and extension, most, access space, et cetera.
01:00 Now I will walk you through the project, so the first part was data loading and data cleaning so here i have imported all the needful libraries so let's take by using the url url, I have requested the dataset and stored it into calgary-access-log.gz then I have tried to open the logs and, method,
01:41 I have read it and in the next step, I have ran a for loop to read each line from the raw logs to match the pattern which is the standard apache common log format i have used I the regular expression to match the pattern then I have divided the data into the needful columns as per the, as per
02:08 the need of assignment and then got, then got the timestamp and date into different, into different columns as well , and handled if there is any lag with zero and, and given, and also specified the format. Next, I have excluded the remote logger name of user.
02:40 and authentication, authentication user, as they are not needed for the analysis. Then, I have appended the data, into a list of dictionaries.
03:04 and then for status i have used int data type and for bytes also i have used int data type and if there is any int string then it will be converted into,  zero, in, for the bytes column.
03:25 Rest of it, I have converted afterwards. Then, I have also, I have used a data frame here.
03:35 As, I have used data frame because it is, more flexible, fast and easy to explain. Then, I have, for the need, as per the need of the project, I have extracted the extension, extension from the filename. Ok, so in the next step I have changed the data type of host, method, file name, extension, date
04:30 to string as string has more methods than the object which is the default data type and then i have also replaced the blanks with, pd.na in the filename as pandas doesn't recognize the, recognize, or does not automatically consider the missing, blanks, so we have to treat them. So, this is all
05:00 about the data cleaning and loading and then in the next part we have the analysis. So, as per the question, we need count of total inaugural reports, so to get the total inaugural reports, I will use the del function and update the data on df, so we got, our, so we got our, reports. Then in the
05:32 second, question, count of unique posts, so the, so as we can see there are only two unique posts, use the ununique function on the host column to get the count with the unique name count. And then in the third question, data has unique filename count so, as you can see, this groups data by date
06:08 and counts how many unique accessed daily, so, from this we can, see which date there was most, most, there there was most use. And in the next version we have got the number of 404 codes which on it.
06:45 So, we got the total to be 23,531, so, these are, these are the times the user, has tried to access the long, and then in the next question we have got the top 15 filenames with 404 response and so the file, from here you can tell which is the missing file and the pages are requested most often by
07:22 the user, m, so, in, for this, I have used the df, m, m, and the status column, m, which had 404 and then grouped it on the filename and and then I reset the index to reset the index on count so that I can sort the values in the descending order as per the need. And then, m, I have used the
08:04 list function to get, get the result as a list, m, of tuple and in, for that I have also used the seed function which binds them together, so, so from here you can analyze, m, the windows.html has the most number of 404, m, error. And then question, we have got a top 15 extension with 404 response
08:44 , so as we have, as I have, as I have extracted the extension from the file name I have, so we, I can use it directly here. So, and then I have grouped it, grouped by the extension, so from, so from here and then use size function for the number of rows and reset it, reset the count, reset the, reset
09:39 the text to, sort the, sort the values in the descending order and again use the list and zip function to bind them and get the list of tuple.
09:49 So from here you can analyze that html extension has a number 404 error and then gif and then Thanks. So in the next question, we have total bandwidth transfer transferred per day.
10:08 So in the month of July, 1955, so for that I used my and group by on the date and the sum of bytes transferred each day. So you can see the transfer of bytes. So we can see the transfer of bytes here. And the next, the next question, we have hourly request distribution, so, so that we can know at which
10:52 hour there is. For that, I used group by on timestamp. Group by on timestamp, of an extended hour.
11:11 So it will be grouped by on the hour and size function to count the number of rows at that timestamp at that hour.
11:23 So, so from that, we can see, at which hour, there was most activity, at which hour the traffic hit the most. So, this gives us the most accessed page by the user, from which we can identify what is the most valuable or most popular content.
12:00 So, for that, I will group by file name and then counted the number of rows and again use the count, reset index to and reset the index on count column and then, and then sorted it in the descending, list and zip function to bind them.
12:25 at the most popular content or was the most valuable file name. In the next, we can, on the next, http response code distribution we can analyze that, most responses were, of 200. , so from this we can see that most of them were using the dot size function and then converted it to a
13:20 dictionary, where key, where the key would be the, key would be the the, um, number of, would be the count of, request made. So from this analysis, from this analysis the challenges I faced was, to capture the dog format and handling the missing values and well-found lines while. You know
14:00 the file name and date function, , this was the two major challenges and then making sure that the data types were right for analysis. From especially server logs for doing analysis. So here I have done practical experience, here I have gained practical experience on data cleaning,
14:40 on regular basis function and understanding the traffic patterns and errors, so that is it, thank you for watching, bye.